{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vncorenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'datasets/vlsp2018_hotel/1-VLSP2018-SA-Hotel-train.csv'\n",
    "VAL_PATH = 'datasets/vlsp2018_hotel/2-VLSP2018-SA-Hotel-dev.csv'\n",
    "TEST_PATH = 'datasets/vlsp2018_hotel/3-VLSP2018-SA-Hotel-test.csv'\n",
    "\n",
    "PRETRAINED_MODEL = 'vinai/phobert-base'\n",
    "MODEL_NAME = 'HotelReviewPhoBert'\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 25\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessors.vlsp2018_preprocessping import VLSP2018Loader\n",
    "\n",
    "raw_datasets = VLSP2018Loader.load(TRAIN_PATH, VAL_PATH, TEST_PATH)\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess and Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessors.vietnamese_preprocessing import VietnameseTextPreprocessor\n",
    "\n",
    "# You should be carefull when using single word replacement for acronyms, because it can cause misinterpretation.\n",
    "# For example, 'giá': ['price', 'gia'] can replace the word 'gia' in 'gia đình', making it become 'giá đình'.\n",
    "vn_preprocessor = VietnameseTextPreprocessor(vncorenlp_dir='../processors/VnCoreNLP', extra_teencodes={\n",
    "    'khách sạn': ['ks'], 'nhà hàng': ['nhahang'], 'nhân viên': ['nv'],\n",
    "    'cửa hàng': ['store', 'sop', 'shopE', 'shop'],\n",
    "    'sản phẩm': ['sp', 'product'], 'hàng': ['hàg'],\n",
    "    'giao hàng': ['ship', 'delivery', 'síp'], 'đặt hàng': ['order'],\n",
    "    'chuẩn chính hãng': ['authentic', 'aut', 'auth'], 'hạn sử dụng': ['date', 'hsd'],\n",
    "    'điện thoại': ['dt'],  'facebook': ['fb', 'face'],\n",
    "    'nhắn tin': ['nt', 'ib'], 'trả lời': ['tl', 'trl', 'rep'],\n",
    "    'feedback': ['fback', 'fedback'], 'sử dụng': ['sd'], 'xài': ['sài'],\n",
    "}, max_correction_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "tokens = tokenizer.encode('Tôi là sinh_viên trường đại_học Công_nghệ thông_tin .')\n",
    "print('Encode:', tokens, '\\nDecode:', tokenizer.decode(tokens))\n",
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_datasets = VLSP2018Loader.preprocess_and_tokenize(raw_datasets, vn_preprocessor, tokenizer, BATCH_SIZE * 2, max_length=MAX_LENGTH)\n",
    "preprocessed_datasets.save_to_disk(\"../datasets/preprocessed_hotel\")\n",
    "display(preprocessed_datasets)\n",
    "pd.DataFrame({\n",
    "    'raw_datasets': raw_datasets['train']['Review'][1480:1490],\n",
    "    'encoded_input_ids': preprocessed_datasets['train']['input_ids'][1480:1490],\n",
    "    'decoded_input_ids': [tokenizer.decode(preprocessed_datasets['train'][i]['input_ids']) for i in range(1480, 1490)]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Extra Steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "preprocessed_datasets = load_from_disk('../datasets/preprocessed_hotel')\n",
    "preprocessed_datasets = VLSP2018Loader.labels_to_flatten_onehot(preprocessed_datasets)\n",
    "preprocessed_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Aspect#Category,Polarity in One-hot form of the first review:')\n",
    "example_onehot = preprocessed_datasets['train'][0]['FlattenOneHotLabels']\n",
    "np.array(example_onehot).reshape(-1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for Tensorflow Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASPECT_CATEGORY_NAMES = raw_datasets['train'].column_names[1:]\n",
    "steps_per_epoch = len(preprocessed_datasets['train']) // BATCH_SIZE\n",
    "total_steps = EPOCHS * steps_per_epoch\n",
    "print(ASPECT_CATEGORY_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf_dataset = preprocessed_datasets['train'].to_tf_dataset(\n",
    "    columns=tokenizer.model_input_names, label_cols='FlattenOneHotLabels',\n",
    "    batch_size=BATCH_SIZE, shuffle=True, num_workers=8\n",
    ")\n",
    "val_tf_dataset = preprocessed_datasets['val'].to_tf_dataset(\n",
    "    columns=tokenizer.model_input_names, label_cols='FlattenOneHotLabels',\n",
    "    batch_size=BATCH_SIZE, shuffle=False, num_workers=8\n",
    ")\n",
    "test_tf_dataset = preprocessed_datasets['test'].to_tf_dataset(\n",
    "    columns=tokenizer.model_input_names, label_cols='FlattenOneHotLabels',\n",
    "    batch_size=BATCH_SIZE, shuffle=False, num_workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "optimizer = Adam(learning_rate=CosineDecay(\n",
    "    initial_learning_rate = 1e-4,\n",
    "    warmup_target = 2e-4,\n",
    "    warmup_steps = int(total_steps * 0.15), # 15% of total_steps\n",
    "    decay_steps = int(total_steps * 0.3), # Next 30% of total_steps\n",
    "    alpha = 0.1, # Minimum lr for decay as a fraction of initial_learning_rate\n",
    "))\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    patience = 3, # Stop if no improvement after 3 epochs\n",
    "    restore_best_weights = True, # You may obtain HIGHER metrics on the test set with longer training time if you set this to False\n",
    "    # Because after some experiments, I found that even with higher val_loss, it still results in better metric reports on the test set. \n",
    "    # This maybe because the training set and the test set have more similarities than the validation data.\n",
    "    # But I think this is not fair, as we already have prior knowledge about the test set and we modified our training based on the performance on this set. \n",
    "    # In real-world, we should only modify our training based on the performance on the validation data\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from acsa_model import VLSP2018MultiTask\n",
    "from helper import plot_training_history\n",
    "model = VLSP2018MultiTask(PRETRAINED_MODEL, ASPECT_CATEGORY_NAMES, optimizer, name=MODEL_NAME)\n",
    "\n",
    "history = model.fit(\n",
    "    train_tf_dataset,\n",
    "    validation_data = val_tf_dataset,\n",
    "    callbacks = [early_stop_callback],\n",
    "    epochs = EPOCHS,\n",
    "    verbose = 1\n",
    ").history\n",
    "\n",
    "model.save_weights(f'./weights/{MODEL_NAME}/{MODEL_NAME}', save_format='tf')\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from acsa_model import VLSP2018MultiTask\n",
    "reloaded_model = VLSP2018MultiTask(PRETRAINED_MODEL, ASPECT_CATEGORY_NAMES, optimizer, name=MODEL_NAME)\n",
    "reloaded_model.load_weights(f'./weights/{MODEL_NAME}/{MODEL_NAME}') # Reload the Model\n",
    "reloaded_model.evaluate(val_tf_dataset, batch_size=BATCH_SIZE, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reloaded_model.acsa_predict(test_tf_dataset, BATCH_SIZE) # On test set\n",
    "reloaded_model.evaluate(test_tf_dataset, batch_size=BATCH_SIZE, verbose=1)\n",
    "print('Example:', raw_datasets['test'][0]['Review'])\n",
    "reloaded_model.print_acsa_pred(y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.data import Dataset # On a random review by user\n",
    "random_input = VLSP2018Loader.preprocess_and_tokenize(\n",
    "    input('Enter your review: '), vn_preprocessor, tokenizer,\n",
    "    batch_size=1, max_length=MAX_LENGTH\n",
    ")\n",
    "tf_inputs = Dataset.from_tensor_slices({x: [[random_input[x][0]]] for x in tokenizer.model_input_names})\n",
    "random_pred = reloaded_model.acsa_predict(tf_inputs)\n",
    "reloaded_model.print_acsa_pred(random_pred[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import argmax_label_matrix\n",
    "y_test = argmax_label_matrix(preprocessed_datasets['test']['FlattenOneHotLabels'])\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluators.sklearn_evaluator import VLSP2018SklearnEvaluator\n",
    "sk_eval = VLSP2018SklearnEvaluator(y_test, y_pred, ASPECT_CATEGORY_NAMES)\n",
    "# sk_eval.report(report_type='Aspect#Category,Polarity').round(3)\n",
    "# sk_eval.report(report_type='Aspect#Category').round(3)\n",
    "# sk_eval.report(report_type='Polarity').round(3)\n",
    "# sk_eval.report(report_type='macro_avg').round(3)\n",
    "sk_eval.report(report_type='all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
